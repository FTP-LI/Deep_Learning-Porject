import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import matplotlib.pyplot as plt
import matplotlib
from torch.utils.data import DataLoader, Dataset
from Unet import UNet
from Unet_Attention import AttentionUNet
import copy
from tqdm import tqdm
import os
import torchvision.transforms as transforms
from torch.optim.lr_scheduler import ReduceLROnPlateau
from scipy.ndimage import gaussian_filter, map_coordinates
import cv2

# è®¾ç½®matplotlibä¸­æ–‡å­—ä½“ï¼Œè§£å†³ä¸­æ–‡ä¹±ç é—®é¢˜
import platform
import warnings
import matplotlib.font_manager as fm

# æŠ‘åˆ¶æ‰€æœ‰matplotlibç›¸å…³çš„è­¦å‘Š
warnings.filterwarnings('ignore', category=UserWarning, module='matplotlib')
warnings.filterwarnings('ignore', category=UserWarning, module='matplotlib.font_manager')
warnings.filterwarnings('ignore', message='.*does not have a glyph.*')
warnings.filterwarnings('ignore', message='.*substituting with a dummy symbol.*')

# æŠ‘åˆ¶å­—ä½“è­¦å‘Š
warnings.filterwarnings('ignore', category=UserWarning, module='matplotlib')

# å°è¯•è®¾ç½®ä¸­æ–‡å­—ä½“ï¼Œå¦‚æœå¤±è´¥åˆ™ä½¿ç”¨è‹±æ–‡
try:
    # Windowsç³»ç»Ÿå­—ä½“è®¾ç½®
    if platform.system() == 'Windows':
        matplotlib.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'Arial Unicode MS']
    elif platform.system() == 'Darwin':  # macOS
        matplotlib.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'Hiragino Sans GB']
    else:  # Linux
        matplotlib.rcParams['font.sans-serif'] = ['WenQuanYi Micro Hei', 'DejaVu Sans']
    
    # å…³é”®ï¼šå¼ºåˆ¶ç¦ç”¨unicodeè´Ÿå·ï¼Œä½¿ç”¨ASCIIè´Ÿå·
    matplotlib.rcParams['axes.unicode_minus'] = False
    matplotlib.rcParams['font.size'] = 10
    
    # é¢å¤–è®¾ç½®ä»¥é¿å…å­—ä½“è­¦å‘Š
    matplotlib.rcParams['font.family'] = 'sans-serif'
    matplotlib.rcParams['mathtext.fontset'] = 'dejavusans'  # æ•°å­¦æ–‡æœ¬å­—ä½“
    matplotlib.rcParams['mathtext.default'] = 'regular'     # æ•°å­¦æ–‡æœ¬æ ·å¼
    
    # æµ‹è¯•ä¸­æ–‡å­—ä½“æ˜¯å¦å¯ç”¨
    import matplotlib.pyplot as plt
    fig, ax = plt.subplots()
    ax.text(0.5, 0.5, 'æµ‹è¯•', fontsize=12)
    plt.close(fig)
    
    USE_CHINESE = True
    print("âœ“ ä¸­æ–‡å­—ä½“è®¾ç½®æˆåŠŸ")
    
except Exception as e:
    # å¦‚æœä¸­æ–‡å­—ä½“è®¾ç½®å¤±è´¥ï¼Œä½¿ç”¨è‹±æ–‡æ ‡ç­¾
    matplotlib.rcParams['font.sans-serif'] = ['DejaVu Sans', 'Arial']
    matplotlib.rcParams['axes.unicode_minus'] = False
    matplotlib.rcParams['mathtext.fontset'] = 'dejavusans'
    matplotlib.rcParams['mathtext.default'] = 'regular'
    USE_CHINESE = False
    print("âš  ä¸­æ–‡å­—ä½“ä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨è‹±æ–‡æ ‡ç­¾")

# é¢å¤–çš„è­¦å‘ŠæŠ‘åˆ¶
import logging
logging.getLogger('matplotlib.font_manager').setLevel(logging.ERROR)
logging.getLogger('matplotlib').setLevel(logging.ERROR)

# å¼ºåˆ¶é‡å»ºå­—ä½“ç¼“å­˜
try:
    fm._rebuild()
except:
    pass

# æ›´å¼ºåˆ¶æ€§çš„ä¸­æ–‡å­—ä½“è®¾ç½®
USE_CHINESE = False

try:
    # æ¸…é™¤ç°æœ‰å­—ä½“è®¾ç½®
    matplotlib.rcdefaults()
    
    # æ ¹æ®ç³»ç»Ÿè®¾ç½®å­—ä½“
    if platform.system() == 'Windows':
        # Windowsç³»ç»Ÿå¸¸è§ä¸­æ–‡å­—ä½“
        font_candidates = ['SimHei', 'Microsoft YaHei', 'SimSun', 'KaiTi', 'FangSong']
    elif platform.system() == 'Darwin':  # macOS
        font_candidates = ['Arial Unicode MS', 'Hiragino Sans GB', 'PingFang SC']
    else:  # Linux
        font_candidates = ['WenQuanYi Micro Hei', 'Noto Sans CJK SC', 'DejaVu Sans']
    
    # æ£€æŸ¥å¯ç”¨å­—ä½“
    available_fonts = [f.name for f in fm.fontManager.ttflist]
    chinese_font = None
    
    for font in font_candidates:
        if font in available_fonts:
            chinese_font = font
            break
    
    if chinese_font:
        # è®¾ç½®ä¸­æ–‡å­—ä½“
        matplotlib.rcParams['font.sans-serif'] = [chinese_font, 'DejaVu Sans']
        matplotlib.rcParams['axes.unicode_minus'] = False
        matplotlib.rcParams['font.size'] = 10
        
        # å¼ºåˆ¶è®¾ç½®æ‰€æœ‰ç›¸å…³å‚æ•°
        matplotlib.rcParams['font.family'] = 'sans-serif'
        matplotlib.rcParams['axes.titlesize'] = 14
        matplotlib.rcParams['axes.labelsize'] = 12
        matplotlib.rcParams['legend.fontsize'] = 12
        
        # æµ‹è¯•ä¸­æ–‡æ˜¾ç¤º
        fig, ax = plt.subplots(figsize=(1, 1))
        ax.text(0.5, 0.5, 'æµ‹è¯•ä¸­æ–‡', fontsize=12, ha='center')
        ax.set_title('æµ‹è¯•æ ‡é¢˜')
        plt.close(fig)
        
        USE_CHINESE = True
        print(f"âœ“ æˆåŠŸè®¾ç½®ä¸­æ–‡å­—ä½“: {chinese_font}")
    else:
        raise Exception("æœªæ‰¾åˆ°å¯ç”¨çš„ä¸­æ–‡å­—ä½“")
        
except Exception as e:
    # å¦‚æœä¸­æ–‡å­—ä½“è®¾ç½®å¤±è´¥ï¼Œä½¿ç”¨è‹±æ–‡
    matplotlib.rcParams['font.sans-serif'] = ['DejaVu Sans', 'Arial', 'Liberation Sans']
    matplotlib.rcParams['axes.unicode_minus'] = False
    matplotlib.rcParams['font.size'] = 10
    USE_CHINESE = False
    print(f"âš  ä¸­æ–‡å­—ä½“è®¾ç½®å¤±è´¥: {str(e)}")
    print("å°†ä½¿ç”¨è‹±æ–‡æ ‡ç­¾æ˜¾ç¤ºå›¾è¡¨")

# é¢å¤–çš„å­—ä½“ç¼“å­˜æ¸…ç†
try:
    import matplotlib.pyplot as plt
    plt.rcdefaults()
    if USE_CHINESE and platform.system() == 'Windows':
        plt.rcParams['font.sans-serif'] = [chinese_font if 'chinese_font' in locals() else 'SimHei']
        plt.rcParams['axes.unicode_minus'] = False
except:
    pass

# ==================== è®­ç»ƒé…ç½®å‚æ•° ====================
# å¯ç›´æ¥ä¿®æ”¹çš„è®­ç»ƒå‚æ•°
# ä¿®æ”¹è®­ç»ƒé…ç½®å‚æ•°
TOTAL_EPOCHS = 100          # æ€»è®­ç»ƒè½®æ¬¡
BATCH_SIZE = 1              # æ‰¹æ¬¡å¤§å°
LEARNING_RATE = 0.0005      # é™ä½å­¦ä¹ ç‡ä»0.001åˆ°0.0005
WEIGHT_DECAY = 1e-4         # æƒé‡è¡°å‡
EARLY_STOP_PATIENCE = 20    # å¢åŠ æ—©åœè€å¿ƒå€¼ä»15åˆ°20
LR_SCHEDULER_PATIENCE = 8   # å‡å°‘å­¦ä¹ ç‡è°ƒåº¦è€å¿ƒå€¼ä»10åˆ°8
PRINT_FREQ = 5              # å¢åŠ æ‰“å°é¢‘ç‡

PRINT_FREQ = 10             # æ‰“å°é¢‘ç‡ï¼ˆæ¯å¤šå°‘è½®æ‰“å°ä¸€æ¬¡å­¦ä¹ ç‡ï¼‰

# è·å–è„šæœ¬æ‰€åœ¨ç›®å½•
script_dir = os.path.dirname(os.path.abspath(__file__))

# ä½¿ç”¨ç»å¯¹è·¯å¾„åŠ è½½æ•°æ®
traindata = np.load(os.path.join(script_dir, "å¤„ç†å¥½çš„æ•°æ®é›†", "trainingdataset.npy"), allow_pickle=True)
testdata = np.load(os.path.join(script_dir, "å¤„ç†å¥½çš„æ•°æ®é›†", "testdataset.npy"), allow_pickle=True)

print(f"è®­ç»ƒæ•°æ®é›†å¤§å°: {len(traindata)}")
print(f"æµ‹è¯•æ•°æ®é›†å¤§å°: {len(testdata)}")
print(f"æ€»è®­ç»ƒè½®æ¬¡: {TOTAL_EPOCHS}")
print(f"æ‰¹æ¬¡å¤§å°: {BATCH_SIZE}")

# æŸå¤±å‡½æ•°å®šä¹‰
class DiceLoss(nn.Module):
    def __init__(self, smooth=1):
        super(DiceLoss, self).__init__()
        self.smooth = smooth
    
    def forward(self, pred, target):
        pred = torch.sigmoid(pred)
        intersection = (pred * target).sum()
        dice = (2. * intersection + self.smooth) / (pred.sum() + target.sum() + self.smooth)
        return 1 - dice

class FocalLoss(nn.Module):
    def __init__(self, alpha=1, gamma=2):
        super(FocalLoss, self).__init__()
        self.alpha = alpha
        self.gamma = gamma
    
    def forward(self, pred, target):
        pred = torch.sigmoid(pred)
        ce_loss = F.binary_cross_entropy(pred, target, reduction='none')
        pt = torch.where(target == 1, pred, 1 - pred)
        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss
        return focal_loss.mean()

class CombinedLoss(nn.Module):
    def __init__(self, dice_weight=0.5, focal_weight=0.3, bce_weight=0.2):
        super(CombinedLoss, self).__init__()
        self.dice_weight = dice_weight
        self.focal_weight = focal_weight
        self.bce_weight = bce_weight
        self.dice_loss = DiceLoss()
        self.focal_loss = FocalLoss()
        self.bce_loss = nn.BCEWithLogitsLoss()
    
    def forward(self, pred, target):
        dice = self.dice_loss(pred, target)
        focal = self.focal_loss(pred, target)
        bce = self.bce_loss(pred, target)
        return self.dice_weight * dice + self.focal_weight * focal + self.bce_weight * bce

# æ•°æ®å¢å¼ºç±»
class DataAugmentation:
    def __init__(self):
        self.transforms = transforms.Compose([
            transforms.RandomHorizontalFlip(p=0.5),
            transforms.RandomVerticalFlip(p=0.5),
            transforms.RandomRotation(degrees=15),
        ])
    
    def __call__(self, image, mask):
        # åº”ç”¨ç›¸åŒçš„å˜æ¢åˆ°å›¾åƒå’Œæ©ç 
        seed = torch.randint(0, 2**32, (1,)).item()
        torch.manual_seed(seed)
        image = self.transforms(image)
        torch.manual_seed(seed)
        mask = self.transforms(mask)
        return image, mask

# æ•°æ®åº“åŠ è½½
class Dataset(Dataset):
    def __init__(self, data, augment=False):
        self.len = len(data)
        self.x_data = []
        self.y_data = []
        self.augment = augment
        self.augmentation = DataAugmentation() if augment else None
        
        for item in data:
            x, y = item[0], item[1]
            # ç¡®ä¿æ•°æ®ç±»å‹æ­£ç¡®
            if isinstance(x, np.ndarray):
                x = torch.from_numpy(x.astype(np.float32))
            else:
                x = torch.tensor(x, dtype=torch.float32)
                
            if isinstance(y, np.ndarray):
                y = torch.from_numpy(y.astype(np.float32))
            else:
                y = torch.tensor(y, dtype=torch.float32)
                
            self.x_data.append(x)
            self.y_data.append(y)

    def __getitem__(self, index):
        x, y = self.x_data[index], self.y_data[index]
        
        # åº”ç”¨æ•°æ®å¢å¼ºï¼ˆä»…è®­ç»ƒæ—¶ï¼‰
        if self.augment and self.augmentation:
            x, y = self.augmentation(x, y)
            
        return x, y

    def __len__(self):
        return self.len

# æ•°æ®åº“dataloader
Train_dataset = Dataset(traindata, augment=True)  # è®­ç»ƒæ—¶ä½¿ç”¨æ•°æ®å¢å¼º
Test_dataset = Dataset(testdata, augment=False)   # æµ‹è¯•æ—¶ä¸ä½¿ç”¨æ•°æ®å¢å¼º
dataloader = DataLoader(Train_dataset, batch_size=BATCH_SIZE, shuffle=True)
testloader = DataLoader(Test_dataset, batch_size=BATCH_SIZE, shuffle=False)

# è®­ç»ƒè®¾å¤‡é€‰æ‹©GPUè¿˜æ˜¯CPU
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
print(f"ä½¿ç”¨è®¾å¤‡: {device}")

# æ¨¡å‹åˆå§‹åŒ–
model = AttentionUNet(1, 1)  # è¾“å…¥1é€šé“ï¼Œè¾“å‡º1é€šé“
model.to(device)

# æŸå¤±å‡½æ•°é€‰æ‹© - ä½¿ç”¨ç»„åˆæŸå¤±å‡½æ•°
criterion = CombinedLoss()
criterion.to(device)

# ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨
optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)
scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=LR_SCHEDULER_PATIENCE, verbose=True, min_lr=1e-6)

train_loss = []
test_loss = []

# è®­ç»ƒå‡½æ•°
# è®°å½•å®é™…å­¦ä¹ ç‡å˜åŒ–
lr_history = []  # æ·»åŠ å­¦ä¹ ç‡å†å²è®°å½•

# ä¿®æ”¹ç¬¬300-320è¡Œçš„è®­ç»ƒå‡½æ•°
def train(epoch, total_epochs):
    model.train()
    total_loss = 0
    num_batches = len(dataloader)
    
    train_bar = tqdm(dataloader, desc=f'ç¬¬{epoch:2d}/{total_epochs}è½®-è®­ç»ƒ', 
                     ncols=100, leave=True)
    
    for batch_idx, data in enumerate(train_bar):
        datavalue, datalabel = data
        datavalue, datalabel = datavalue.to(device), datalabel.to(device)
        
        optimizer.zero_grad()
        
        # è·å–æ¨¡å‹è¾“å‡º
        outputs = model(datavalue)
        
        # å¤„ç†æ·±åº¦ç›‘ç£è¾“å‡º
        if isinstance(outputs, tuple):  # è®­ç»ƒæ¨¡å¼ï¼Œæœ‰æ·±åº¦ç›‘ç£
            main_output, aux1, aux2 = outputs
            
            # è®¡ç®—ä¸»æŸå¤±
            main_loss = criterion(main_output, datalabel)
            
            # è®¡ç®—è¾…åŠ©æŸå¤±
            aux_loss1 = criterion(aux1, datalabel) * 0.4
            aux_loss2 = criterion(aux2, datalabel) * 0.2
            
            # æ€»æŸå¤±
            loss = main_loss + aux_loss1 + aux_loss2
        else:  # æ¨ç†æ¨¡å¼
            # åœ¨trainå‡½æ•°ä¸­ï¼Œå°†æ·±åº¦ç›‘ç£æƒé‡å¤§å¹…é™ä½
            if isinstance(outputs, tuple):
                main_output, aux1, aux2 = outputs
                main_loss = criterion(main_output, labels)
                aux_loss1 = criterion(aux1, labels)
                aux_loss2 = criterion(aux2, labels)
                # å¤§å¹…é™ä½è¾…åŠ©æŸå¤±æƒé‡ï¼Œé¿å…è¿‡åº¦çº¦æŸ
                loss = main_loss + 0.1 * aux_loss1 + 0.05 * aux_loss2  # ä»0.4å’Œ0.2é™ä½åˆ°0.1å’Œ0.05
            else:
                loss = criterion(outputs, labels)
        
        # æ£€æŸ¥æŸå¤±æ˜¯å¦å¼‚å¸¸
        if torch.isnan(loss) or torch.isinf(loss) or loss.item() > 10.0:
            print(f"âš ï¸ æ£€æµ‹åˆ°å¼‚å¸¸æŸå¤±å€¼: {loss.item():.6f}ï¼Œè·³è¿‡æ­¤æ‰¹æ¬¡")
            continue
            
        loss.backward()
        
        # æ›´ä¸¥æ ¼çš„æ¢¯åº¦è£å‰ª
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)
        
        optimizer.step()
        total_loss += loss.item()
        
        # æ›´æ–°è¿›åº¦æ¡
        train_bar.set_postfix({
            'Loss': f'{loss.item():.6f}',
            'Avg': f'{total_loss/(batch_idx+1):.6f}'
        })
    
    avg_loss = total_loss / num_batches
    train_loss.append(avg_loss)
    
    # è®°å½•å­¦ä¹ ç‡
    current_lr = optimizer.param_groups[0]['lr']
    lr_history.append(current_lr)
    
    if epoch % 10 == 0 or epoch == 1:
        print(f"ç¬¬{epoch:3d}è½® - è®­ç»ƒæŸå¤±: {avg_loss:.6f}, å­¦ä¹ ç‡: {current_lr:.2e}")
    
    return avg_loss

# æµ‹è¯•å‡½æ•°
# åŒæ ·éœ€è¦ä¿®æ”¹æµ‹è¯•å‡½æ•°æ¥å¤„ç†æ·±åº¦ç›‘ç£è¾“å‡º
def test(epoch, total_epochs):
    model.eval()
    total_loss = 0
    num_batches = len(testloader)
    
    test_bar = tqdm(testloader, desc=f'ç¬¬{epoch:2d}/{total_epochs}è½®-æµ‹è¯•', 
                    ncols=100, leave=True)
    
    with torch.no_grad():
        for batch_idx, data in enumerate(test_bar):
            datavalue, datalabel = data
            datavalue, datalabel = datavalue.to(device), datalabel.to(device)
            
            # è·å–æ¨¡å‹è¾“å‡º
            outputs = model(datavalue)
            
            # æµ‹è¯•æ—¶åªä½¿ç”¨ä¸»è¾“å‡º
            if isinstance(outputs, tuple):
                outputs = outputs[0]  # åªå–ä¸»è¾“å‡º
            
            loss = criterion(outputs, datalabel)
            total_loss += loss.item()
            
            # æ›´æ–°è¿›åº¦æ¡
            test_bar.set_postfix({
                'Loss': f'{loss.item():.6f}',
                'Avg': f'{total_loss/(batch_idx+1):.6f}'
            })
    
    avg_loss = total_loss / num_batches
    test_loss.append(avg_loss)
    
    if epoch % 10 == 0 or epoch == 1:
        print(f"ç¬¬{epoch:3d}è½® - æµ‹è¯•æŸå¤±: {avg_loss:.6f}")
    
    return avg_loss

# æ—©åœæœºåˆ¶
class EarlyStopping:
    def __init__(self, patience=15, min_delta=1e-6):
        self.patience = patience
        self.min_delta = min_delta
        self.counter = 0
        self.best_loss = float('inf')
        
    def __call__(self, val_loss):
        if val_loss < self.best_loss - self.min_delta:
            self.best_loss = val_loss
            self.counter = 0
            return False
        else:
            self.counter += 1
            return self.counter >= self.patience

bestmodel = None
bestepoch = None
bestloss = np.inf
early_stopping = EarlyStopping(patience=EARLY_STOP_PATIENCE)

# è®­ç»ƒå¾ªç¯
print("="*50)
print("å¼€å§‹è®­ç»ƒ...")
print("="*50)

# ä½¿ç”¨é…ç½®çš„æ€»è½®æ¬¡è¿›è¡Œè®­ç»ƒ
for epoch in tqdm(range(1, TOTAL_EPOCHS + 1), desc="æ€»ä½“è®­ç»ƒè¿›åº¦", ncols=100):
    train_loss_epoch = train(epoch, TOTAL_EPOCHS)
    test_loss_epoch = test(epoch, TOTAL_EPOCHS)
    
    # å­¦ä¹ ç‡è°ƒåº¦
    scheduler.step(test_loss_epoch)
    
    # ä¿å­˜æœ€ä½³æ¨¡å‹
    if test_loss_epoch < bestloss:
        bestloss = test_loss_epoch
        bestepoch = epoch
        bestmodel = copy.deepcopy(model)
        print(f"âœ“ æ–°çš„æœ€ä½³æ¨¡å‹ä¿å­˜åœ¨ç¬¬ {epoch} è½®ï¼ŒæŸå¤±: {bestloss:.6f}")
    
    # æ—©åœæ£€æŸ¥
    if early_stopping(test_loss_epoch):
        print(f"\nâš  æ—©åœè§¦å‘ï¼Œåœ¨ç¬¬ {epoch} è½®åœæ­¢è®­ç»ƒ")
        break
    
    # å®šæœŸæ‰“å°å½“å‰å­¦ä¹ ç‡
    if epoch % PRINT_FREQ == 0:
        current_lr = optimizer.param_groups[0]['lr']
        print(f"ğŸ“Š ç¬¬{epoch}è½® - å½“å‰å­¦ä¹ ç‡: {current_lr:.2e}")
    
    print("-" * 50)

print("\n" + "="*50)
print(f"è®­ç»ƒå®Œæˆï¼æœ€ä½³è½®æ¬¡: {bestepoch}, æœ€ä½³æŸå¤±: {bestloss:.6f}")
print("="*50)

# ä¿å­˜æ¨¡å‹
print("æ­£åœ¨ä¿å­˜æ¨¡å‹...")
torch.save(model.state_dict(), os.path.join(script_dir, "è®­ç»ƒå¥½çš„æ¨¡å‹", "lastmodel.pt"))
if bestmodel is not None:
    torch.save(bestmodel.state_dict(), os.path.join(script_dir, "è®­ç»ƒå¥½çš„æ¨¡å‹", "bestmodel.pt"))
print("æ¨¡å‹ä¿å­˜å®Œæˆï¼")

# ä¿å­˜è®­ç»ƒç»“æœå›¾ï¼ˆæ ¹æ®å­—ä½“å¯ç”¨æ€§é€‰æ‹©è¯­è¨€ï¼‰
print("æ­£åœ¨ç”Ÿæˆè®­ç»ƒç»“æœå›¾...")
plt.figure(figsize=(14, 10))

# æ ¹æ®å­—ä½“å¯ç”¨æ€§è®¾ç½®æ ‡ç­¾
if USE_CHINESE:
    train_label = 'è®­ç»ƒæŸå¤±'
    test_label = 'æµ‹è¯•æŸå¤±'
    xlabel_1 = 'è®­ç»ƒè½®æ¬¡'
    ylabel_1 = 'æŸå¤±å€¼'
    title_1 = 'è®­ç»ƒå’Œæµ‹è¯•æŸå¤±æ›²çº¿'
    best_label = f'æœ€ä½³è½®æ¬¡: {bestepoch}' if bestepoch else ''
    lr_label = 'å­¦ä¹ ç‡'
    xlabel_2 = 'è®­ç»ƒè½®æ¬¡'
    ylabel_2 = 'å­¦ä¹ ç‡'
    title_2 = 'å­¦ä¹ ç‡è°ƒåº¦æ›²çº¿'
else:
    train_label = 'Training Loss'
    test_label = 'Validation Loss'
    xlabel_1 = 'Epoch'
    ylabel_1 = 'Loss'
    title_1 = 'Training and Validation Loss Curves'
    best_label = f'Best Epoch: {bestepoch}' if bestepoch else ''
    lr_label = 'Learning Rate'
    xlabel_2 = 'Epoch'
    ylabel_2 = 'Learning Rate'
    title_2 = 'Learning Rate Schedule'

# ç¬¬ä¸€ä¸ªå­å›¾ï¼šæŸå¤±æ›²çº¿
plt.subplot(2, 1, 1)
plt.plot(range(1, len(train_loss) + 1), train_loss, label=train_label, color='blue', linewidth=2)
plt.plot(range(1, len(test_loss) + 1), test_loss, label=test_label, color='red', linewidth=2)
plt.legend(fontsize=12)
plt.xlabel(xlabel_1, fontsize=12)
plt.ylabel(ylabel_1, fontsize=12)
plt.title(title_1, fontsize=14, fontweight='bold')
plt.grid(True, alpha=0.3)

# æ ‡è®°æœ€ä½³ç‚¹
if bestepoch is not None:
    plt.axvline(x=bestepoch, color='green', linestyle='--', alpha=0.7, label=best_label)
    plt.legend(fontsize=12)

# ç¬¬äºŒä¸ªå­å›¾ï¼šå­¦ä¹ ç‡æ›²çº¿
plt.subplot(2, 1, 2)
if len(lr_history) > 0:
    plt.plot(range(1, len(lr_history) + 1), lr_history, label=lr_label, color='orange', linewidth=2)
else:
    # å¤‡ç”¨æ˜¾ç¤º
    lr_values = [LEARNING_RATE] * len(train_loss)
    plt.plot(range(1, len(lr_values) + 1), lr_values, label=lr_label, color='orange', linewidth=2)

plt.xlabel(xlabel_2, fontsize=12)
plt.ylabel(ylabel_2, fontsize=12)
plt.title(title_2, fontsize=14, fontweight='bold')
plt.grid(True, alpha=0.3)
plt.yscale('log')
plt.legend(fontsize=12)

# æ·»åŠ æŸå¤±èŒƒå›´é™åˆ¶æ˜¾ç¤º
plt.subplot(2, 1, 1)
# é™åˆ¶yè½´èŒƒå›´ä»¥æ›´å¥½åœ°æ˜¾ç¤ºæ­£å¸¸æŸå¤±å˜åŒ–
max_loss = max(max(train_loss), max(test_loss))
if max_loss > 5.0:  # å¦‚æœæœ€å¤§æŸå¤±è¿‡å¤§ï¼Œé™åˆ¶æ˜¾ç¤ºèŒƒå›´
    plt.ylim(0, min(max_loss, 2.0))
    plt.text(0.02, 0.98, f'æ³¨æ„ï¼šéƒ¨åˆ†æŸå¤±å€¼è¶…å‡ºæ˜¾ç¤ºèŒƒå›´\næœ€å¤§æŸå¤±: {max_loss:.2f}', 
             transform=plt.gca().transAxes, fontsize=10, 
             verticalalignment='top', bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.7))

plt.plot(range(1, len(train_loss) + 1), train_loss, label=train_label, color='blue', linewidth=2)
plt.plot(range(1, len(test_loss) + 1), test_loss, label=test_label, color='red', linewidth=2)
plt.legend(fontsize=12)
plt.xlabel(xlabel_1, fontsize=12)
plt.ylabel(ylabel_1, fontsize=12)
plt.title(title_1, fontsize=14, fontweight='bold')
plt.grid(True, alpha=0.3)

# ä¿å­˜å›¾ç‰‡
save_path = os.path.join(script_dir, 'è®­ç»ƒå¥½çš„æ¨¡å‹', 'training_results.png')
plt.tight_layout()  # è°ƒæ•´å¸ƒå±€
plt.savefig(save_path, dpi=300, bbox_inches='tight')  # ä¿å­˜é«˜è´¨é‡å›¾ç‰‡

plt.show()

print("è®­ç»ƒç»“æœå›¾ç”Ÿæˆå®Œæˆï¼")
print(f"å›¾ç‰‡ä¿å­˜ä½ç½®: {save_path}")
print("="*50)
print("è®­ç»ƒå®Œæˆï¼")
